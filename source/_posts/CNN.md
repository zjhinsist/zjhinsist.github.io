---
title: CNN
date: 2022-07-12 16:24:06
tags: CNN
description: CNN讲解
background: url(https://s3.bmp.ovh/imgs/2022/08/08/e1c9bda47806a33d.png)
categories: 人工智能
---

#  CNN卷积神经网络

卷积神经网络与传统网络的区别：

一个是三维的，一个是二维的

![image-20220712164546322](https://s2.loli.net/2022/11/04/s6AqVhElcTokaQU.png)

 

## 整体架构

![image-20220712165411609](https://s2.loli.net/2022/11/04/rIAqZyx5kdpYhv8.png)



## 卷积的计算方法：

先把图像分为一个个不同的小区域，然后在通过权重矩阵，算出每一个小区域的值

![image-20220712170141035](https://s2.loli.net/2022/11/04/JTRwk7jQDBiretg.png)

 



计算方法： 特征矩阵 * 权重矩阵

![image-20220712173505215](https://s2.loli.net/2022/11/04/QtoXHjD8xluwiAh.png)



卷积一般需要做三次以上：

![image-20220712173715307](https://s2.loli.net/2022/11/04/UcT89KPo2Gyre6m.png)



堆叠的卷积层：

![image-20220712174052965](https://s2.loli.net/2022/11/04/PM1faqtjmAkuY96.png)

## 卷积的参数

![image-20220712174331194](https://s2.loli.net/2022/11/04/Cx9UjbKf4SIQ6Ml.png)

### 步长

![image-20220712174420577](https://s2.loli.net/2022/11/04/lANMS4ybzaB1XeO.png)

### 卷积核尺寸

卷积核尺寸越小，我们所得到的特征越明显，卷积核尺寸越大，我们得到特征越模糊，一般最小卷积核的尺寸就是3 * 3的

### 边缘填充

我们在计算卷积的时候，越到边缘的地方，它的影响越小，如果想要改变这个现状，我们需要进行边缘填充

![image-20220712181751440](https://s2.loli.net/2022/11/04/Df8NAazLRWqy1ZH.png)

从这个步长为2的卷积中，我们可以看出，边缘的数据，只进行了一次乘法，而中间的数据进行了两次乘法，这时候边缘和中间的数据，对最后的影响就会不同，我们可以通过边缘填充来消除这个影响

![image-20220712181925662](https://s2.loli.net/2022/11/04/4tEzTOYGuQnlJhx.png)

由图中可以看出灰色的为边缘填充的部分，紫色的为我们的目标地址，

## 卷积计算公式



![image-20220713200907832](https://s2.loli.net/2022/11/04/1Xh7uWGlUHAMyaJ.png)

卷积参数共享，一般一个卷积核使用同一套参数即可，不用每一个区域我们都要改变参数



## 池化层

当我们采集的特征过多的时候，这时候我们可以使用池化层来进行特征压缩：

![image-20220714074722943](https://s2.loli.net/2022/11/04/pPhwr5CLaQB49V3.png)

框选每一个区域，得到其中特征最明显的一个



池化方法：最大池化



![image-20220714075107698](https://s2.loli.net/2022/11/04/baODNk6isV93UY8.png)



## 感受野

![image-20220714081535111](https://s2.loli.net/2022/11/04/rRIpobHMEGz4kPq.png)

就是通过卷积之后的一个特征，是通过前面多少个特征计算得来的

例如上面的图像，第一层是5 * 5，第二层是3 * 3， 第三层是1 * 1，我们可以看出最后这一个卷积，是通过前面5 * 5的计算得出的，所以这它的感受野就是 5 * 5

通俗点就是：特征图上的一个点对应输入图上的区域 



如果堆积3个3 * 3的卷积层，并且保持滑动窗口步长为1，其感受野就是7 * 7 的，这跟使用一个7 * 7 的卷积核的结果是相同的，那么为什么非要堆叠3 个小卷积那？

![image-20220714082742948](https://s2.loli.net/2022/11/04/zXMTNAUWRQJbkdm.png)



## 经典网络

### Alexnet网络

提取特征的卷积核和步长比较大特征提取比较模糊，层数也比较小，所以现在这个网络不常用

![image-20220717082026475](https://s2.loli.net/2022/11/04/RFYruNx8sMgT94H.png)



### Vgg网络

Vgg网络相较于Alexnet网络，卷积核的大小变小了，都是3 * 3，这就表明了在提取特征的时候Vgg比较细腻，而且该网络的层数，也相较于Alexnet网络有了较大的增长，而且Vgg在每一层之间，进行了Maxpool(最大池化)之后，在下一层的时候使其特征数量翻倍，来解决Maxpool带来的特征减少的情况

![image-20220717082049592](https://s2.loli.net/2022/11/04/1CMpzVdFgJZkWA5.png)



### 残差网络Resnet

当我们使用神经网络来处理事件的时候，如果我们一味增加网络的深度，最后得到的结果反倒不如网络深度较浅的网络的结果，这主要是因为，我们在进行网络层数加深的时候，我们不能够确定这一层的处理的结果就一定比上一层好，这时候就出现了残差网络Resnet，残差网络中定义，如果这一次的训练结果，没有上一层好的话我们就将这一层的权重参数设置为零，跳过这一层

![image-20220717082646627](https://s2.loli.net/2022/11/04/wa7zVotk1FNSvDU.png)



## 迁移学习

我们在训练模型的时候，往往可以借助别人已经训练好的参数，在这个参数的基础上进行在训练，改进和提高，

![image-20220717194200624](https://s2.loli.net/2022/11/04/47d1eHo6NDGtWTR.png)

我们在使用别人训练好的数据的 时候有两种方案：

1. 我们把别人训练好的参数当做我们这个模型的初始化条件，然后我们在进行我们的训练
2. 我们直接把别人训练好的参数拿来用几乎不做改变，就是把别人的卷积层拿来使用，全连接层自己调节
